{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra points to add\n",
    "#source, telescope, freq, flux, flux error, beam diam, date\n",
    "\n",
    "#Herrnstein et al, 1998, APJ, 497:L69\n",
    "#doesn't specify day\n",
    "#these are strict brightnesses without beam considerations\n",
    "#drop VLBI observations\n",
    "#extra1=['NGC4258','VLBA',22,'.220*','NA',.00045,'1997/03/01']\n",
    "\n",
    "#Doi et al, 2005, MNRAS, 363:2\n",
    "extra2=['NGC4258','SCUBA-JCMT',347,93.7,29.3,15,'NA', 'Doi et al, 2005, MNRAS, 363:2']\n",
    "extra3=['NGC4258','NMA',96,11,2.7,7,'NA', 'Doi et al, 2005, MNRAS, 363:2']\n",
    "\n",
    "#Kamali, 2017, A\\&A, 605:A84\n",
    "#beam size for these sources do not give reliable beam sizes ranging from 0.19 to 0.50 arcsec '0.19-0.50'\n",
    "extra4=['ESO558-G009','VLA',33,.80,.04,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "extra5=['IC2560','VLA',33, 2.00,0.10,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "extra6=['Mrk1029','VLA',33,1.18,0.14,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "extra7=['NGC1194','VLA',33,1.08,0.04,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "extra8=['NGC2273','VLA',33,2.69,0.29,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "extra9=['NGC3393','VLA',33,5.30,0.36,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "extra10=['NGC4388','VLA',33,8.57,0.43,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "extra11=['NGC5495','VLA',33,0.13,0.02,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "extra12=['UGC3789','VLA',33,0.21,0.02,'0.19-0.50','NA','Kamali, 2017, A&A, 605:A84']\n",
    "\n",
    "#Krips, 2006, A\\&A: 446:113\n",
    "#doesn't state the date '2003/02/01'\n",
    "#drop VLBI observations\n",
    "#extra13=['NGC1068','IRAM PdB',100,36,0.4,.21,'2003/02/01']\n",
    "#extra14=['NGC1068','IRAM PdB',300,22,0.8,0.8,'2003/02/01']\n",
    "\n",
    "extras=[]\n",
    "\n",
    "#extras.append(extra1)\n",
    "\n",
    "extras.append(extra2)\n",
    "extras.append(extra3)\n",
    "\n",
    "extras.append(extra4)\n",
    "extras.append(extra5)\n",
    "extras.append(extra6)\n",
    "extras.append(extra7)\n",
    "extras.append(extra8)\n",
    "extras.append(extra9)\n",
    "extras.append(extra10)\n",
    "extras.append(extra11)\n",
    "extras.append(extra12)\n",
    "\n",
    "#extras.append(extra13)\n",
    "#extras.append(extra14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def normalize_row(row):\n",
    "    # NEW: handle list-of-1 where the sole element is a space-separated line\n",
    "    if isinstance(row, list) and len(row) == 1 and isinstance(row[0], str):\n",
    "        s = row[0].strip()\n",
    "        # if it looks like a bracketed list, let your existing logic handle it later\n",
    "        if not (s.startswith('[') and s.endswith(']')):\n",
    "            # split on 2+ spaces to keep things like '0.2*' intact\n",
    "            parts = [p.strip() for p in re.split(r'\\s{2,}', s) if p.strip()]\n",
    "            return parts\n",
    "\n",
    "    # If row is a list of string fragments like [\"'A',\", \"'B',\", ...] -> join and eval FIRST\n",
    "    if isinstance(row, list) and all(isinstance(s, str) for s in row) and any(\",\" in s for s in row):\n",
    "        try:\n",
    "            row = ast.literal_eval(''.join(row))\n",
    "        except Exception:\n",
    "            # fall through; we'll at least clean strings\n",
    "            pass\n",
    "\n",
    "    # Now clean fields (note: NO comma stripping here)\n",
    "    if isinstance(row, list):\n",
    "        clean = []\n",
    "        for s in row:\n",
    "            if isinstance(s, str):\n",
    "                s = s.strip().strip(\"'\").strip('\"').replace(\"\\\\n\", \"\").strip()\n",
    "            clean.append(s)\n",
    "        return clean\n",
    "    else:\n",
    "        s = str(row).strip().strip(\"'\").strip('\"').replace(\"\\\\n\", \"\").strip()\n",
    "        return [s]\n",
    "\n",
    "\n",
    "\n",
    "def date_key(s, default=-1.0):\n",
    "    if s is None:\n",
    "        return default\n",
    "    s = str(s).strip().strip(\"'\").strip('\"').strip(',')\n",
    "    if s.upper() == \"NA\" or s == \"\":\n",
    "        return default\n",
    "    parts = s.split(\"/\")\n",
    "    if len(parts) != 3:\n",
    "        return default\n",
    "    try:\n",
    "        y, m, d = [float(p.strip().strip(\"'\").strip('\"').strip(',')) for p in parts]\n",
    "        return y + m/12.0 + d/365.0\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "\n",
    "def safe_float(s):\n",
    "    # Numeric or 'NA'\n",
    "    try:\n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return -1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sort the data summary\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "os.chdir('analysis/multi_freq_from_archive')\n",
    "\n",
    "all_lines=[]\n",
    "\n",
    "textin=f'fitsumfiles/sorted/fitsummary_fixtochosen_sorted.txt'\n",
    "textout=f'fitsumfiles/final/fitsummary_final_withextras.txt'\n",
    "\n",
    "if not os.path.exists('fitsumfiles/final'):\n",
    "    os.makedirs('fitsumfiles/final')\n",
    "\n",
    "write=True\n",
    "#write=False\n",
    "\n",
    "totex=0\n",
    "with open(textout, 'w') as file:\n",
    "    if write==True:\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = file\n",
    "    all_lines=[]\n",
    "    with open(textin, 'r') as infile:\n",
    "        tline=[]\n",
    "        for line in infile:\n",
    "            line = line.split(' ')\n",
    "            line2=[]\n",
    "            for i in line:\n",
    "                if i!='':\n",
    "                    line2.append(i)\n",
    "            all_lines.append(line2)\n",
    "    source_order=[]\n",
    "    doneindex=[]\n",
    "    n = len(all_lines)\n",
    "    for i in range(n):\n",
    "        breaker=False\n",
    "        for idoneindex in doneindex:\n",
    "            if i==idoneindex:\n",
    "                breaker=True\n",
    "        if breaker==True:\n",
    "            continue\n",
    "        isource_order=[]\n",
    "        isource_order.append(all_lines[i])\n",
    "        for j in range(n):\n",
    "            if j==1:\n",
    "                continue\n",
    "            if j!=i:\n",
    "                if all_lines[i][0]==all_lines[j][0]:\n",
    "                    #so that while iterating through i the source is not repeated\n",
    "                    isource_order.append(all_lines[j])\n",
    "                    doneindex.append(j)\n",
    "            n2 = len(isource_order)\n",
    "        #sorting by frequency\n",
    "\n",
    "        for iex in extras:\n",
    "            if iex[0] in isource_order[0][0]:\n",
    "                #source(0), telescope(1), freq(2), flux(3), flux error(4), beam diam(5), date(6)\n",
    "                # -> target(0), Telescope(1), freq(2), flux(3), date (index)(6), index: 'NA', beam size (\")(5), snr (3)/(4), reference         \n",
    "                #           0,      1,      2,      3,      6,      5,      3/4 \n",
    "                if len(str(iex[3]).split('*'))==1:\n",
    "                    exnew=[iex[0], iex[1], str(iex[2]), str(iex[3]), iex[6], str(iex[5]), f'{float(iex[3])/float(iex[4]):.3f}', f'{iex[7]}\\n']\n",
    "                else:\n",
    "                    exnew=[iex[0], iex[1], str(iex[2]), str(iex[3]), iex[6], str(iex[5]), 'NA', f'{iex[7]}\\n']\n",
    "                totex=totex+1\n",
    "                isource_order.append(exnew)\n",
    "\n",
    "\n",
    "\n",
    "        # --- normalize then sort ---\n",
    "        isource_order = [normalize_row(row) for row in isource_order]\n",
    "\n",
    "        # (optional) peek\n",
    "        #print(\"FIRST ROW (normalized):\", isource_order[0])\n",
    "\n",
    "        # Apply normalization and skip empty rows\n",
    "        isource_order = [\n",
    "            normalize_row(row)\n",
    "            for row in isource_order\n",
    "            if row and any(str(item).strip() for item in row)  # skip if row is empty or only blanks\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "        for i2source_order in isource_order:\n",
    "            print(i2source_order)\n",
    "            pass\n",
    "        print('\\n')\n",
    "    if write==True:\n",
    "        sys.stdout = original_stdout\n",
    "if totex!=len(extras)-1:\n",
    "    input('Extras are not all being read') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make machine readable table\n",
    "totspacefind=0\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "textin='fitsumfiles/final/fitsummary_final_withextras.txt'\n",
    "\n",
    "if not os.path.exists('machinetables'):\n",
    "    os.makedirs('machinetables')\n",
    "textout='machinetables/fitsummary_machine.txt'\n",
    "\n",
    "write=True\n",
    "#write=False\n",
    "\n",
    "spacings=[15, 15, 19, 16, 15, 19, 15,112]\n",
    "decimals=['NA','NA',0,1,'NA',5,3]\n",
    "totspace=0\n",
    "for i in spacings:\n",
    "    totspace=totspace+i\n",
    "\n",
    "allbeams=[]\n",
    "allsnr=[]\n",
    "\n",
    "totex=0\n",
    "\n",
    "with open(textout, 'w') as file:\n",
    "    if write==True:\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = file\n",
    "    all_lines=[]\n",
    "    with open(textin, 'r') as infile:\n",
    "        tline=[]\n",
    "        for line in infile:\n",
    "            line = line.split(' ')\n",
    "            line2=[]\n",
    "            for i in line:\n",
    "                if i!='':\n",
    "                    line2.append(i)\n",
    "            all_lines.append(line2)\n",
    "    source_order=[]\n",
    "    doneindex=[]\n",
    "    n = len(all_lines)\n",
    "    for i in range(n):\n",
    "        if i==0:\n",
    "            continue\n",
    "        breaker=False\n",
    "        for idoneindex in doneindex:\n",
    "            if i==idoneindex:\n",
    "                breaker=True\n",
    "        if breaker==True:\n",
    "            continue\n",
    "        isource_order=[]\n",
    "        isource_order.append(all_lines[i])\n",
    "\n",
    "        for j in range(n):\n",
    "            if j==1:\n",
    "                continue\n",
    "            if j!=i:\n",
    "                if all_lines[i][0]==all_lines[j][0]:\n",
    "                    #so that while iterating through i the source is not repeated\n",
    "                    isource_order.append(all_lines[j])\n",
    "                    doneindex.append(j)\n",
    "            n2 = len(isource_order)\n",
    "        #sorting by frequency\n",
    "\n",
    "        for iex in extras:\n",
    "            if iex[0] in isource_order[0][0]:\n",
    "                # source(0), telescope(1), freq(2), flux(3), flux error(4), beam diam(5), date(6)                       \n",
    "                # -> target(0), Telescope(1), freq(2), flux(3), date (6), index: 'NA', beam size (\")(5), snr (3)/(4)    \n",
    "                #           0,      1,      2,      3,      6,     'NA',     5,      3/4                            \n",
    "                if len(str(iex[3]).split('*'))==1:\n",
    "                    exnew=[iex[0], iex[1], str(iex[2]), str(iex[3]), iex[6],  str(iex[5]), f'{float(iex[3])/float(iex[4]):.3f}',f'{iex[7]}']\n",
    "                else:\n",
    "                    exnew=[iex[0], iex[1], str(iex[2]), str(iex[3]), iex[6],  str(iex[5]), 'NA',f'{iex[7]}']\n",
    "                totex=totex+1\n",
    "                isource_order.append(exnew)\n",
    "\n",
    "        isource_order = [\n",
    "            normalize_row(row)\n",
    "            for row in isource_order\n",
    "            if row and any(str(item).strip() for item in row)  # skip empty or all-blank\n",
    "        ]\n",
    "\n",
    "\n",
    "        isource_order = sorted(\n",
    "            isource_order,\n",
    "            key=lambda x: (\n",
    "                safe_float(x[2]),  # frequency\n",
    "                date_key(x[4]),    # date or NA\n",
    "                safe_float(x[5])   # beam or NA\n",
    "            )\n",
    "        )\n",
    "    \n",
    "\n",
    "        for i2source_order in isource_order:\n",
    "            i2source_order = normalize_row(i2source_order)  # <-- add this\n",
    "            #decimals=['NA','NA',0,2,'NA',5,3]\n",
    "            #freq\n",
    "            freq=float(i2source_order[2])\n",
    "            if freq<10:\n",
    "                new2=f'{freq:.1f}'\n",
    "            else:\n",
    "                new2=f'{freq:.0f}'\n",
    "            #flux\n",
    "            flux=i2source_order[3]\n",
    "            if float(flux.split('*')[0])<0.1:\n",
    "                #input('error')\n",
    "                #input(flux)\n",
    "                pass\n",
    "            if len(flux.split('*'))>1:\n",
    "                flux=float(flux.split('*')[0])\n",
    "                flux=f'{flux:.{1}f}*'\n",
    "            else:\n",
    "                flux=float(flux)\n",
    "                flux=f'{flux:.{1}f}'\n",
    "            new3=flux\n",
    "            beam=i2source_order[5]\n",
    "            if '-'in i2source_order[5]:\n",
    "                beam=i2source_order[5]\n",
    "            else:\n",
    "                if not  isinstance(beam, float):\n",
    "                    beam=float(i2source_order[5])\n",
    "                if beam<0.1:\n",
    "                    beam=f'{beam:.4f}'\n",
    "                elif beam<1:\n",
    "                    beam=f'{beam:.3f}'\n",
    "                elif beam<10:\n",
    "                    beam=f'{beam:.2f}'\n",
    "                elif beam<100:\n",
    "                    beam=f'{beam:.1f}'\n",
    "                else:\n",
    "                    input(f'problem with beam {beam}')\n",
    "            \n",
    "            allbeams.append(beam)\n",
    "            new5=beam\n",
    "            #snr\n",
    "            snr=i2source_order[6]\n",
    "            snr=snr.replace(\"\\n\", \"\")\n",
    "            if snr!='NA':\n",
    "                allsnr.append(snr)\n",
    "                snr=float(i2source_order[6])\n",
    "                snr=f'{snr:.0f}'\n",
    "            new6=snr\n",
    "            \n",
    "            new7=i2source_order[7].replace(\"\\n\", \"\")\n",
    "            if '/' in new7:\n",
    "                new7=new7.split('/')[2]\n",
    "            if 'Gaussian_Fit_Pipeline' in new7:\n",
    "                new7='Burridge, 2025'\n",
    "            if len(new7)> totspacefind:\n",
    "                totspacefind=len(new7)\n",
    "                newlonval=new7\n",
    "            print(f\"{i2source_order[0].ljust(spacings[0])}{i2source_order[1].ljust(spacings[1])}{new2.ljust(spacings[2])}{new3.ljust(spacings[3])}{i2source_order[4].ljust(spacings[4])}{new5.ljust(spacings[5])}{new6.ljust(spacings[6])}{new7.ljust(spacings[7])}\")\n",
    "    \n",
    "\n",
    "    if write==True:\n",
    "        sys.stdout = original_stdout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the length of the longest file name\n",
    "print(totspacefind)\n",
    "print(newlonval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run scripts to get graphs, contamination\n",
    "import nbformat\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "\n",
    "with open(\"Beam_Date_Flux_Graphs.ipynb\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "ep = ExecutePreprocessor(timeout=600)\n",
    "ep.preprocess(nb, {\"metadata\": {\"path\": \"./\"}})\n",
    "\n",
    "with open(\"Variability_Search.ipynb\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "ep = ExecutePreprocessor(timeout=600)\n",
    "ep.preprocess(nb, {\"metadata\": {\"path\": \"./\"}})\n",
    "\n",
    "with open(\"flux_size_graph.ipynb\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "ep = ExecutePreprocessor(timeout=600)\n",
    "ep.preprocess(nb, {\"metadata\": {\"path\": \"./\"}})\n",
    "\n",
    "with open(\"get_contams.ipynb\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "ep = ExecutePreprocessor(timeout=600)\n",
    "ep.preprocess(nb, {\"metadata\": {\"path\": \"./\"}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Conda base)",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
